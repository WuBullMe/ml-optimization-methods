# Model configuration
model:
  name_class: "NetModel"
  params:
    num_classes: 10

# Optimization techniques
optimization:
  name_class: "QuantizationOptimization"
  QuantizationModule: 
    quantization_method: "pqt_static"
    quantization_params:
      calibration_steps: 200
      calibration_batch_size: 32
      qat_epochs: 10
    bits: 8
    layers: ["Linear", "Conv2d"]

    optimizer:
      name: "Adam"
      params:
        lr: 0.00005

    scheduler:
      name: "LambdaLR"
      params:
        lr_lambda: "lambda epoch: 0.9 ** epoch"

    loss: "cross_entropy"
    
  train:
    max_epochs: 10

  
  callbacks:
    - class: "ModelCheckpoint"
      params:
        monitor: "val_loss"
        mode: "min"
        save_top_k: 3
        filename: "best-{epoch}-{val_loss:.2f}"
        auto_insert_metric_name: false
        save_last: true

    - class: "LearningRateMonitor"
      params:
        logging_interval: "epoch"

  logging:
    logger: "TensorBoard"
    params:
      save_dir: "./logs"
      name: "quantization_pqt_static_netmodel_cifar10"
      version: "1"
      default_hp_metric: false

    log_every_n_steps: 1
  
  hardware:
    accelerator: "gpu"
    devices: 1
    strategy: "auto"
    benchmark: true

# Data configuration
dataset:
  - name_class: "Cifar10DataLoader"
    name: "CIFAR10"
    root: "./data"
    split: "train"
    download: True
    batch_size: 128
    num_workers: 4
    transforms:
      train:
        Resize:
          size: !!python/tuple [224, 224]
        RandomHorizontalFlip:
          p: 0.5
        ToTensor: {}
        Normalize:
          mean: [0.4914, 0.4822, 0.4465]
          std: [0.2023, 0.1994, 0.2010]

  - name_class: "Cifar10DataLoader"
    name: "CIFAR10"
    root: "./data"
    split: "val"
    download: True
    batch_size: 256
    num_workers: 4
    pin_memory: True
    size: 1000
    transforms:
      val:
        Resize:
          size: !!python/tuple [224, 224]
        ToTensor: {}
        Normalize:
          mean: [0.4914, 0.4822, 0.4465]
          std: [0.2023, 0.1994, 0.2010]